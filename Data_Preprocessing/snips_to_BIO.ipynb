{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKA6u46J1cKf",
        "outputId": "2d9399bf-9a71-4aee-8e95-962fed877322"
      },
      "source": [
        "path1 = \"/content/seq_in.txt\"\n",
        "path2 = \"/content/seq_out.txt\"\n",
        "path3 = \"/content/dev.txt\"\n",
        "\n",
        "with open(path1,'r') as f1:\n",
        "  with open(path2,'r') as f2:\n",
        "    with open(path3,'w') as wf:\n",
        "      text = f1.readlines()\n",
        "      #print(text)\n",
        "      labels = f2.readlines()\n",
        "      print(len(text))\n",
        "      print(len(labels))\n",
        "      for i in range(len(text)):\n",
        "        line = text[i]\n",
        "        tag = labels[i]\n",
        "        #print(line)\n",
        "        line = line.replace(\"     \",\" \")\n",
        "        line = line.replace(\"    \",\" \")\n",
        "        line = line.replace(\"   \",\" \")\n",
        "        line = line.replace(\"  \",\" \")\n",
        "        tag = tag.replace(\"  \",\" \")\n",
        "        line_list = line.strip(\"\\n\").split(\" \")\n",
        "        #print(tag)\n",
        "        tag_list = tag.split(\" \")\n",
        "        #print(line_list,len(line_list))\n",
        "        #print(tag_list,len(tag_list))\n",
        "        if(len(tag_list)==len(line_list)+1):\n",
        "          for j in range(len(line_list)):\n",
        "            wf.write(line_list[j]+\" \"+tag_list[j]+\"\\n\")\n",
        "          wf.write(\"\\n\")\n",
        "        elif(len(tag_list)==len(line_list)):\n",
        "          for j in range(len(line_list)-1):\n",
        "            wf.write(line_list[j]+\" \"+tag_list[j]+\"\\n\")\n",
        "          wf.write(\"\\n\")\n",
        "        else:\n",
        "          print(line_list,len(line_list))\n",
        "          print(tag_list,len(tag_list))\n",
        "          print(\"error\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "700\n",
            "700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmRjUtnCDK60",
        "outputId": "817207c0-62b2-4785-8190-92ae605c05f2"
      },
      "source": [
        "path2 = \"/content/seq_out.txt\"\n",
        "tag_set = set()\n",
        "with open(path2,'r') as f2:\n",
        "  labels = f2.readlines()\n",
        "  for i in range(len(labels)):\n",
        "    tag = labels[i]\n",
        "    tag_list = tag.split(\" \")\n",
        "    #print(tag_list)\n",
        "    for j in range(len(tag_list)):\n",
        "      tag_set.add(tag_list[j])\n",
        "print(tag_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-music_item', '\\n', 'B-city', 'I-track', 'B-playlist', 'I-spatial_relation', 'I-restaurant_type', 'B-state', 'B-movie_type', 'I-sort', 'B-timeRange', 'B-object_location_type', 'I-object_location_type', 'B-cuisine', 'B-condition_description', 'I-movie_name', 'I-entity_name', 'I-playlist', 'I-country', 'B-movie_name', 'B-object_name', 'I-location_name', 'B-genre', 'I-object_name', 'I-service', 'B-rating_value', 'B-current_location', 'I-object_select', 'B-spatial_relation', 'B-poi', 'B-artist', 'B-sort', 'I-served_dish', 'B-geographic_poi', 'I-object_type', 'I-poi', 'I-movie_type', 'I-state', 'I-artist', 'B-track', 'B-facility', 'I-current_location', 'B-object_select', 'B-served_dish', 'I-music_item', 'B-object_type', 'B-condition_temperature', 'B-object_part_of_series_type', 'B-entity_name', 'B-country', 'B-party_size_number', 'I-cuisine', 'I-album', 'O', 'B-service', 'B-party_size_description', 'I-geographic_poi', 'B-best_rating', 'B-restaurant_type', 'B-location_name', 'B-rating_unit', 'B-album', 'I-genre', 'B-playlist_owner', 'B-year', 'I-timeRange', 'I-restaurant_name', 'B-restaurant_name', 'I-playlist_owner', 'I-party_size_description', 'I-city'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BxBQwjQF23o",
        "outputId": "72a7bd21-9d00-4602-a32b-54e4462bec69"
      },
      "source": [
        "path2 = \"/content/seq_out.txt\"\n",
        "with open(path2,'r') as f2:\n",
        "  labels = f2.readlines()\n",
        "  for i in range(len(labels)):\n",
        "    tag = labels[i]\n",
        "    tag_list = tag.split(\" \")\n",
        "    #print(tag_list)\n",
        "    for j in range(len(tag_list)):\n",
        "      tag_set.add(tag_list[j])\n",
        "print(tag_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-music_item', '\\n', 'B-city', 'I-track', 'B-playlist', 'I-spatial_relation', 'I-restaurant_type', 'B-state', 'B-movie_type', 'I-sort', 'B-timeRange', 'B-object_location_type', 'I-object_location_type', 'B-cuisine', 'B-condition_description', 'I-movie_name', 'I-entity_name', 'I-playlist', 'I-country', 'B-movie_name', 'B-object_name', 'I-location_name', 'B-genre', 'I-object_part_of_series_type', 'I-object_name', 'I-service', 'B-rating_value', 'B-current_location', 'I-object_select', 'B-spatial_relation', 'B-poi', 'B-artist', 'B-sort', 'I-served_dish', 'B-geographic_poi', 'I-object_type', 'I-poi', 'I-movie_type', 'I-state', 'I-artist', 'B-track', 'B-facility', 'I-current_location', 'B-object_select', 'B-served_dish', 'I-music_item', 'B-object_type', 'B-condition_temperature', 'B-object_part_of_series_type', 'B-entity_name', 'B-country', 'B-party_size_number', 'I-cuisine', 'I-album', 'O', 'B-service', 'B-party_size_description', 'I-geographic_poi', 'B-best_rating', 'B-restaurant_type', 'B-location_name', 'B-rating_unit', 'B-album', 'I-facility', 'I-genre', 'B-playlist_owner', 'B-year', 'I-timeRange', 'I-restaurant_name', 'B-restaurant_name', 'I-playlist_owner', 'I-party_size_description', 'I-city'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "306WXBn-NX2H"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.load('/content/y_data_2592.npy',allow_pickle=True)\n",
        "\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_YZTmtvNtmA"
      },
      "source": [
        "!cat /content/tag_ids_2592.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13NqZ0XfPvYd"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def read_turks(file):\n",
        "    '''Load in DataTurks formatted tsv file and output array of each entry'''\n",
        "    with open(file) as f:\n",
        "        lines = [i.rstrip().split(\"\\t\") for i in f.readlines()]\n",
        "    return lines\n",
        "\n",
        "def clean_words(word_ents):\n",
        "    '''removes quote and comma characters from '''\n",
        "    new_word_ents = []\n",
        "    for ents in word_ents:\n",
        "        word = ents[0]\n",
        "        if word.find(',') > 0:\n",
        "            word = word[word.find(',')+1:]\n",
        "        word = word.replace('\"','')\n",
        "        ents[0] = word\n",
        "        new_word_ents.append(ents)\n",
        "    return new_word_ents\n",
        "\n",
        "def create_seqs(word_ents):\n",
        "    '''Formats word entities as sequences'''\n",
        "    seqs = []\n",
        "    seq = []\n",
        "    for ents in word_ents:\n",
        "        if len(ents)>1:\n",
        "            seq.append(ents)\n",
        "        else:\n",
        "            seqs.append(seq)\n",
        "            seq=[]\n",
        "    return seqs\n",
        "\n",
        "def add_iob_scheme(word_ents):\n",
        "    '''adds IOB scheme to tags'''\n",
        "    new_ents = []\n",
        "    for i in range(0,len(word_ents)):\n",
        "        if word_ents[i][1] == \"O\":\n",
        "            tag = word_ents[i][1]\n",
        "        else:\n",
        "            if not i:\n",
        "                tag = \"B-\"+word_ents[i][1]\n",
        "                #print(tag)\n",
        "            else:\n",
        "                if (word_ents[i][1] != word_ents[i-1][1]):\n",
        "                    tag = \"B-\"+word_ents[i][1]\n",
        "                else:\n",
        "                    tag = \"I-\"+word_ents[i][1]\n",
        "\n",
        "        new_ents.append([word_ents[i][0],tag])\n",
        "    return new_ents\n",
        "\n",
        "def pad_seq(seq,max_len):\n",
        "    '''pads or truncates a sequence to a specified length'''\n",
        "    padded_seq = seq+[[\"<PAD>\",\"O\"]]*max_len\n",
        "    return padded_seq[:max_len]\n",
        "\n",
        "def pad_sequences(sequences,max_len=None):\n",
        "    '''pads or truncates a list of sequences to a specified length'''\n",
        "    if max_len == None:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "    return [pad_seq(seq,max_len) for seq in sequences]\n",
        "\n",
        "def get_word_ids(sentances,tag=False):\n",
        "    ''''Enumerates each unique word or tag in the dataset and creates\n",
        "        a mapping for each word/tag number pair'''\n",
        "\n",
        "    words = []\n",
        "    for sentance in sentances:\n",
        "        words += list([word[tag] for word in sentance])\n",
        "    word_dict = {word:i for i,word in enumerate(set(words))}\n",
        "    return word_dict\n",
        "\n",
        "def words_to_ids(sentances,word_ids,tag_ids):\n",
        "    '''Maps each sequence to its word/tag id representation'''\n",
        "    vector = []\n",
        "    for sentance in sentances:\n",
        "        vector.append(list([[word_ids[w[0]],tag_ids[w[1]]] for w in sentance]))\n",
        "    return np.array(vector)\n",
        "\n",
        "def create_x_y(matrix,n_tags):\n",
        "    '''Formats sequences as features(x) and labels(y)'''\n",
        "    x = []\n",
        "    y = []\n",
        "    for sequences in matrix:\n",
        "        xi = [i[0] for i in sequences]\n",
        "        yi = [i[1] for i in sequences]\n",
        "        x.append(xi)\n",
        "        y.append(yi)\n",
        "    y = np.array([to_categorical(i,n_tags) for i in y])\n",
        "    return np.array(x),y\n",
        "\n",
        "def save_ids(word_ids,tag_ids,output_path,n_samples):\n",
        "    np.save(f\"{output_path}word_ids_{n_samples}.npy\",word_ids)\n",
        "    np.save(f\"{output_path}tag_ids_{n_samples}.npy\",tag_ids)\n",
        "    print('-'*40)\n",
        "    print(\"~~~Word/Tag Ids Successfully Saved~~~\")\n",
        "    return\n",
        "\n",
        "def save_vals(x,y,file_path = \"annotations/\"):\n",
        "    '''Saves x and y values to a specified location'''\n",
        "    np.save(f\"{file_path}x_data_{len(x)}.npy\",x)\n",
        "    np.save(f\"{file_path}y_data_{len(y)}.npy\",y)\n",
        "    print(\"-\"*40)\n",
        "    print(\"~~~Values Successfully Saved~~~\")\n",
        "    print(\"-\"*40)\n",
        "    print(\"X-shape:\",x.shape)\n",
        "    print(\"Sample:\")\n",
        "    print(x[0][:5])\n",
        "    print('-'*40)\n",
        "    print(\"Y-shape:\",y.shape)\n",
        "    print(\"Sample:\")\n",
        "    print(y[0][:5])\n",
        "    print(\"-\"*40)\n",
        "    return\n",
        "\n",
        "def create_dataset(turks_file,output_filepath):\n",
        "    path = \"/content/train.txt\"\n",
        "    word_ents = read_turks(turks_file)\n",
        "    new_ents = clean_words(word_ents)\n",
        "    #print(new_ents)\n",
        "    #print(len(new_ents))\n",
        "    seqs = create_seqs(new_ents)\n",
        "    iob_tag_seqs = [add_iob_scheme(ents) for ents in seqs]\n",
        "    #print(iob_tag_seqs)\n",
        "    with open(path,'w') as wf:\n",
        "      for line in iob_tag_seqs:\n",
        "        #print(line)\n",
        "        for word in line:\n",
        "          wf.write(word[0]+\" \"+word[1]+\"\\n\")\n",
        "          #print(word[0],word[1])\n",
        "        wf.write(\"\\n\")\n",
        "    \"\"\"padded_seqs = pad_sequences(iob_tag_seqs,max_len=50)\n",
        "    word_ids = get_word_ids(padded_seqs)\n",
        "    tag_ids = get_word_ids(padded_seqs,tag=True)\n",
        "    vectors = words_to_ids(padded_seqs,word_ids,tag_ids)\n",
        "    n_tags = len(tag_ids)\n",
        "    x,y = create_x_y(vectors,n_tags)\n",
        "    save_ids(word_ids,tag_ids,output_filepath,len(x))\n",
        "    save_vals(x,y,output_filepath)\"\"\"\n",
        "    return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    turks_file = \"/content/MedicalNERDataset2600.tsv\"\n",
        "    output_filepath = \"/content/annotations/\"\n",
        "    create_dataset(turks_file,output_filepath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0BNP4zXbdVH"
      },
      "source": [
        "path = \"/content/train.txt\"\n",
        "path1 = \"/content/train1.txt\"\n",
        "\n",
        "with open(path,'r') as f:\n",
        "  with open(path1,'w') as wf:\n",
        "    text = f.readlines()\n",
        "    for line in text:\n",
        "      if(line.startswith(\" O\")==0):\n",
        "        #print(line)\n",
        "        wf.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}