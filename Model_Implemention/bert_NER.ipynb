{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRc-I5mp0k6k"
      },
      "source": [
        "Plan1 BERT-BiLSTM-CRF-NER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiv0Wu-uMKp4",
        "outputId": "efab0005-954f-47e3-9515-020cf82e4fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.python.org/simple\n",
            "Collecting bert-base==0.0.9\n",
            "  Downloading bert_base-0.0.9-py3-none-any.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting GPUtil>=1.3.0\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-base==0.0.9) (1.15.0)\n",
            "Requirement already satisfied: pyzmq>=16.0.0 in /usr/local/lib/python3.7/dist-packages (from bert-base==0.0.9) (22.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from bert-base==0.0.9) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bert-base==0.0.9) (1.19.5)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7410 sha256=576ff0343712a7b8c7944347229832abf63c4ae8c690f2611e186fa31ae349ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built GPUtil\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 113, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4849, in parseImpl\n",
            "    loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1716, in _parseNoCache\n",
            "    tokens = fn(instring, tokensStart, retTokens)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1316, in wrapper\n",
            "    ret = func(*args[limit[0]:])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 81, in <lambda>\n",
            "    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/markers.py\", line 307, in __init__\n",
            "    self._markers = _coerce_parse_result(MARKER.parseString(marker))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4052, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4070, in parseImpl\n",
            "    if exprtokens or exprtokens.haskeys():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 643, in __bool__\n",
            "    return (not not self.__toklist)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 212, in _main\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1424, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1614, in isEnabledFor\n",
            "    def isEnabledFor(self, level):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!pip install bert-base==0.0.9 -i https://pypi.python.org/simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySRu9JIgQB_6",
        "outputId": "130f19b1-92ce-45c6-c427-48b3a7f514cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BERT-BiLSTM-CRF-NER'...\n",
            "remote: Enumerating objects: 385, done.\u001b[K\n",
            "remote: Total 385 (delta 0), reused 0 (delta 0), pack-reused 385\u001b[K\n",
            "Receiving objects: 100% (385/385), 3.85 MiB | 26.46 MiB/s, done.\n",
            "Resolving deltas: 100% (211/211), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/macanv/BERT-BiLSTM-CRF-NER\n",
        "\n",
        "#!python3 setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lEf5AYNdQPe2",
        "outputId": "23d356f9-4f6b-4d85-e988-9e932e3a54fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8efmSc2kItY3",
        "outputId": "06df4549-2532-4a11-e10f-6427c76b61ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ynHsJcjQQsc",
        "outputId": "45399513-42fb-420a-de25-405982affac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BERT-BiLSTM-CRF-NER\n"
          ]
        }
      ],
      "source": [
        "cd /content/BERT-BiLSTM-CRF-NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zIwqnm3QS25"
      },
      "outputs": [],
      "source": [
        "!python3 setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFOaqYRB-ACd",
        "outputId": "13321857-20c6-444b-8533-2a92fd85c34e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.15.0\n",
            "Uninstalling tensorflow-2.15.0:\n",
            "  Successfully uninstalled tensorflow-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70m6xXMScx8y"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhY4H5Be6Zsj",
        "outputId": "ff736e57-1f5a-4509-f994-5990409ea466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-23 16:16:44--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.24.207, 142.251.10.207, 142.250.4.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.24.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  21.8MB/s    in 18s     \n",
            "\n",
            "2024-04-23 16:17:03 (21.2 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrUeSsDk3Khy"
      },
      "outputs": [],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKWdcGmN3NDG",
        "outputId": "667813e2-39de-44ce-81b6-37120b424147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BERT-BiLSTM-CRF-NER\n"
          ]
        }
      ],
      "source": [
        "cd BERT-BiLSTM-CRF-NER/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E30M0LWu6bt5"
      },
      "outputs": [],
      "source": [
        "!unzip /content/uncased_L-12_H-768_A-12.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jli2mVr9Xgj6"
      },
      "source": [
        "用的是这个"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvPumoYEkVat"
      },
      "source": [
        "'training data is so small...': processor.get_train_examples(args.data_dir), NerProcessor, if len(tokens) == 4:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJru5qfkPksE",
        "outputId": "2ac4ed75-4286-44c9-e28e-b672b78ddef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-23 17:24:10.674488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/bert-base-ner-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('bert-base==0.0.9', 'console_scripts', 'bert-base-ner-train')())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bert_base-0.0.9-py3.10.egg/bert_base/runs/__init__.py\", line 29, in train_ner\n",
            "    from bert_base.train.bert_lstm_ner import train\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bert_base-0.0.9-py3.10.egg/bert_base/train/bert_lstm_ner.py\", line 24, in <module>\n",
            "    from bert_base.bert import optimization\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bert_base-0.0.9-py3.10.egg/bert_base/bert/optimization.py\", line 84, in <module>\n",
            "    class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
            "AttributeError: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'\n"
          ]
        }
      ],
      "source": [
        "!bert-base-ner-train \\\n",
        "    -data_dir /content/BERT-NER/data/ \\\n",
        "    -output_dir /content/output/ \\\n",
        "    -init_checkpoint /content/BERT-NER/cased_L-12_H-768_A-12/bert_model.ckpt \\\n",
        "    -bert_config_file /content/BERT-NER/cased_L-12_H-768_A-12/bert_config.json \\\n",
        "    -vocab_file /content/BERT-NER/cased_L-12_H-768_A-12/vocab.txt \\\n",
        "    -num_train_epochs 5 \\\n",
        "    -batch_size 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJIHj2bNRTQs",
        "outputId": "33fcdd33-a0a0-44d6-e75e-b9546727f3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pHQ1ZUFRO9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223b683a-aed0-4222-fa4d-e112ec045358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-23 17:24:24.394288: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/bert-base-ner-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('bert-base==0.0.9', 'console_scripts', 'bert-base-ner-train')())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bert_base-0.0.9-py3.10.egg/bert_base/runs/__init__.py\", line 29, in train_ner\n",
            "    from bert_base.train.bert_lstm_ner import train\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bert_base-0.0.9-py3.10.egg/bert_base/train/bert_lstm_ner.py\", line 24, in <module>\n",
            "    from bert_base.bert import optimization\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bert_base-0.0.9-py3.10.egg/bert_base/bert/optimization.py\", line 84, in <module>\n",
            "    class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
            "AttributeError: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'\n"
          ]
        }
      ],
      "source": [
        "!bert-base-ner-train -help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eRHZcVmWQyV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "b7f59d5b-e76a-4949-ae3a-45fed6f66f32"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-8-ed532d696c20>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-ed532d696c20>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    -data_dir=/content/BERT-NER/data/   \\\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "!python3 bert_lstm_ner.py   \\\n",
        "  -do_train=True   \\\n",
        "  -do_eval=True   \\\n",
        "  -do_predict=True\n",
        "  -data_dir=/content/BERT-NER/data/   \\\n",
        "  -vocab_file=/content/BERT-NER/cased_L-12_H-768_A-12/vocab.txt  \\\n",
        "  -bert_config_file=/content/BERT-NER/cased_L-12_H-768_A-12/bert_config.json \\\n",
        "  -init_checkpoint=/content/BERT-NER/cased_L-12_H-768_A-12/bert_model.ckpt   \\\n",
        "  -max_seq_length=128   \\\n",
        "  -train_batch_size=32   \\\n",
        "  -learning_rate=2e-5   \\\n",
        "  -num_train_epochs=3.0   \\\n",
        "  -output_dir=/content/output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJekEudSiMIi",
        "outputId": "8ccbf431-ea43-446a-d1e1-c0241a1908fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'usr/local'\n",
            "/usr\n"
          ]
        }
      ],
      "source": [
        "cd usr/local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVI48ASkiVGG",
        "outputId": "a1ec5dc7-66e6-495b-922c-d844797dad7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bert_base/train\n"
          ]
        }
      ],
      "source": [
        "cd train/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPh8avOkiY0b",
        "outputId": "48d4d1df-e352-430c-d5f9-285b0965096f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert_lstm_ner.py  __init__.py        models.py     tf_metrics.py\n",
            "conlleval.py      lstm_crf_layer.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  train_helper.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgNTl70AiyQA",
        "outputId": "ce902e2e-dfca-49ad-fef9-ffed402e8804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: vi: command not found\n"
          ]
        }
      ],
      "source": [
        "!vi bert_lstm_ner.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E1LuL_B0si1"
      },
      "source": [
        "Plan2 BERT-NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VRj12mY_INJ",
        "outputId": "ee4bc14f-8330-4307-c567-674d358b73ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94Oxhcu7Xqfe",
        "outputId": "41a2823d-6c28-4e5a-f2ce-629b2796693c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRL3dq2V0uhN",
        "outputId": "0a4c0962-cb7d-4b49-e747-bad7798df220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BERT-NER'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 196 (delta 13), reused 11 (delta 11), pack-reused 178\u001b[K\n",
            "Receiving objects: 100% (196/196), 2.23 MiB | 3.97 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kyzhouhzau/BERT-NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwzcL71E1OT9",
        "outputId": "4ae88a43-fd1a-4b9b-a994-ce70e9bbacd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BERT-NER\n"
          ]
        }
      ],
      "source": [
        "cd BERT-NER/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BworT952A4w",
        "outputId": "29e170e2-9098-4d24-f6c7-759cdfa4611c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bert'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 328.27 KiB | 25.25 MiB/s, done.\n",
            "Resolving deltas: 100% (182/182), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/google-research/bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxvBVL4W3Wra",
        "outputId": "bf065551-e6ef-4b5e-dc32-665e2778a9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-23 15:46:36--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.10.207, 142.250.4.207, 172.217.194.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.10.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M  17.0MB/s    in 23s     \n",
            "\n",
            "2024-04-23 15:46:59 (16.8 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMQYohgL3pRX",
        "outputId": "272cb3ee-4767-4043-ffe6-2691fd0eb138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/BERT-NER/cased_L-12_H-768_A-12.zip\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/BERT-NER/cased_L-12_H-768_A-12.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r98e7muvEXqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab74beb-0b11-4252-bf71-bce37b709f71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_tJZcEjE2h-",
        "outputId": "ae3be685-e287-451e-df5f-6a027294efc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==2.9.0\n",
            "  Downloading tensorflow_gpu-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/511.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:14\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n",
            "    data = self.__fp.read(amt)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 466, in read\n",
            "    s = self.fp.read(amt)\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1303, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1159, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 397, in resolve\n",
            "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 293, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 225, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 304, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 516, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 587, in _prepare_linked_requirement\n",
            "    local_file = unpack_url(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 166, in unpack_url\n",
            "    file = get_http_url(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 107, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/download.py\", line 147, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install tensorflow-gpu==2.9.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zctDpPiu_LE",
        "outputId": "d21ad217-5856-43a8-df6d-6e1ed92e2886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "VFGXLbgT94_8",
        "outputId": "b72a5273-0ffe-4953-9a63-a6bf933fe521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.20.0\n",
            "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.26.1\n",
            "    Uninstalling protobuf-5.26.1:\n",
            "      Successfully uninstalled protobuf-5.26.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.4.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-aiplatform 1.48.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ac4233c00a70465cb9a525be560762e1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW_wAaxMTt3j"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_umINni9enCD"
      },
      "source": [
        "要先删一下所有的tensorflow, !python -m pip uninstall tensorflow tensorflow-gpu && python -m pip install tensorflow-gpu && python -c \"import tensorflow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-CnVgMyQco6"
      },
      "source": [
        "写一个注意事项，要改的地方很多。首先tensorflow要用1.13.1  \n",
        "BERT_NER.py的229行，labels要改一下  \n",
        "194行，以“.”结尾要去掉，因为我们的数据没有  \n",
        "BERT-NER/output/result_dir要手动建一下这个文件夹  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqR5Pu671Lit",
        "outputId": "48fc53bd-5827-47b8-e22f-a82f560de877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-23 17:19:14.337812: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2024-04-23 17:19:14.337843: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/BERT-NER/BERT_NER.py\", line 18, in <module>\n",
            "    from bert import optimization\n",
            "  File \"/content/BERT-NER/bert/optimization.py\", line 87, in <module>\n",
            "    class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
            "AttributeError: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'\n",
            "processed 0 tokens with 0 phrases; found: 0 phrases; correct: 0.\n"
          ]
        }
      ],
      "source": [
        "!bash run_ner.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQhqBaK056-5"
      },
      "outputs": [],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crRdM3K37JCV"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu==1.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap4Xs7yS5_np",
        "outputId": "92d19e55-1b91-469e-f188-f12ff8050180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow==1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT2oVrDp6sLX"
      },
      "outputs": [],
      "source": [
        "!python -m pip uninstall tensorflow tensorflow-gpu && python -m pip install tensorflow-gpu && python -c \"import tensorflow\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMdRAaL49Vt6"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/BERT-NER/output/result_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAV5MvnCaTDT"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/BERT-NER/output/result_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhKGoRmgq9Eg"
      },
      "source": [
        "处理一些异常数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcmyGJ83q-wB"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/BERT-NER/data/test.txt\",'r') as f:\n",
        "  with open(\"/content/BERT-NER/data/test1.txt\", 'a') as wf:\n",
        "    text = f.readlines()\n",
        "    for line in text:\n",
        "      if(\" \" not in line and line!=\"\\n\"):\n",
        "        print(line)\n",
        "      else:\n",
        "        wf.write(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0eSI2ZUyxj1"
      },
      "source": [
        "处理一些异常数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US7_EOTcyyLX"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/valid.txt\", \"r\") as f:\n",
        "  with open(\"/content/valid1.txt\", \"w\") as wf:\n",
        "    text = f.readlines()\n",
        "    for line in text:\n",
        "      if(line.startswith(\" O\")==0):\n",
        "        #print(line)\n",
        "        wf.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt7KEkyk5kzD",
        "outputId": "1a44d91a-7a56-4679-9b82-a63cf0f12f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm /content/valid1.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7InLTRutT7i",
        "outputId": "08c02b8a-c1d4-451e-b5c8-30343bfa9c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8xWZczjUxp8"
      },
      "outputs": [],
      "source": [
        "!unzip /content/total.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duqcYN27YE4i"
      },
      "outputs": [],
      "source": [
        "!unzip /content/train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-6U2DKIYFqg"
      },
      "outputs": [],
      "source": [
        "!unzip /content/valid.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ez4VFnfYGFX"
      },
      "outputs": [],
      "source": [
        "!unzip /content/test.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTAOmjr_U0Fz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "\n",
        "path = \"/content/test\"\n",
        "path1 = \"/content/test.txt\"\n",
        "cc = 0\n",
        "for file in os.listdir(path):\n",
        "  with open(path + \"/\" + file, \"r\") as f:\n",
        "    with open(path1, 'a') as wf:\n",
        "      text = f.readlines()\n",
        "      for line in text:\n",
        "        new_line = line.strip(\"\\n\")\n",
        "        new_line = new_line.replace(\" _\",\"\")\n",
        "        new_line = new_line.replace(\" B-\",\"\\tB-\")\n",
        "        new_line = new_line.replace(\" I-\",\"\\tI-\")\n",
        "        new_line = new_line.replace(\" O\",\"\\tO\")\n",
        "        new_line = new_line.split(\"\\t\")\n",
        "        #print(new_line)\n",
        "        if(line!=\"\\n\"):\n",
        "          wf.write(new_line[0] + \"\\t\" + new_line[1] + \"\\n\")\n",
        "        else:\n",
        "          wf.write(line)\n",
        "      cc +=1\n",
        "      wf.write(\"\\n\")\n",
        "      print(file)\n",
        "print(cc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfvNVfyeVslP",
        "outputId": "ef9c65be-43a8-4137-fbd5-48748b985760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "成功！\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "root = \"/content/total\"\n",
        "\n",
        "#构建所有文件名的列表，dir为label\n",
        "filename = []\n",
        "#label = []\n",
        "names = os.listdir(root)\n",
        "for n in names:\n",
        "  filename.append(dir_path + '/' + n)\n",
        "\n",
        "#打乱文件名列表\n",
        "np.random.shuffle(filename)\n",
        "#划分训练集、测试集，默认比例4:1\n",
        "train = filename[:int(len(filename)*0.8)]\n",
        "test = filename[int(len(filename)*0.8):]\n",
        "\n",
        "#分别写入train.txt, test.txt\n",
        "with open('train.txt', 'w') as f1, open('test.txt', 'w') as f2:\n",
        "    for i in train:\n",
        "        f1.write(i + '\\n')\n",
        "    for j in test:\n",
        "        f2.write(j + '\\n')\n",
        "\n",
        "print('成功！')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4TilmBkbCIl"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2024.4.23"
      ],
      "metadata": {
        "id": "RfwzwDPIw4D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYM8JRGmw-jV",
        "outputId": "786d939d-54bf-4e33-970c-4bef4ca721bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hertz-pj/BERT-BiLSTM-CRF-NER-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_0YQ6Mhw6gz",
        "outputId": "a509bb46-94ef-4037-830d-a519480283b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BERT-BiLSTM-CRF-NER-pytorch'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 40 (delta 17), reused 35 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (40/40), 18.73 KiB | 18.73 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/BERT-BiLSTM-CRF-NER-pytorch/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU3DtxJOxzz_",
        "outputId": "a47ec09b-d5e9-49b6-fb27-ddd635e2ce7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BERT-BiLSTM-CRF-NER-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/BERT-BiLSTM-CRF-NER-pytorch/cased_L-12_H-768_A-12.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc75Wzdoxpy7",
        "outputId": "a3643d02-2110-40f9-d68b-6551c4b09a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/BERT-BiLSTM-CRF-NER-pytorch/cased_L-12_H-768_A-12.zip\n",
            "   creating: cased_L-12_H-768_A-12/\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUsLapig2e1Q",
        "outputId": "9e10209f-b17f-4351-defa-e5794daa434d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-crf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kFm0inU2jfX",
        "outputId": "16c130c4-1f0e-4fe2-a1a4-102139130221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gbIibnH2o0h",
        "outputId": "af19fe45-4e93-4709-a281-aed1bbace585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.4.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=ee5cd6658c2e828d1e1ae4bff7e49acf5d805fcb8b0b34dcac6d035ed8b315b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpasFdPa2rzC",
        "outputId": "8f61d551-0cb1-4e00-9306-a5bf2c9faeb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvSdlxp32zqw",
        "outputId": "11b87120-3a83-4750-d10b-277469b48446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m174.1/176.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (1.25.2)\n",
            "Collecting boto3 (from pytorch_transformers)\n",
            "  Downloading boto3-1.34.89-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (4.66.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (0.1.99)\n",
            "Collecting sacremoses (from pytorch_transformers)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting botocore<1.35.0,>=1.34.89 (from boto3->pytorch_transformers)\n",
            "  Downloading botocore-1.34.89-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_transformers)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch_transformers)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.89->boto3->pytorch_transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch_transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch_transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.89->boto3->pytorch_transformers) (1.16.0)\n",
            "Installing collected packages: sacremoses, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch_transformers\n",
            "Successfully installed boto3-1.34.89 botocore-1.34.89 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch_transformers-1.2.0 s3transfer-0.10.1 sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f38z0h43h2k",
        "outputId": "284677d0-2517-4f2c-e81e-cc97b197e359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-23 16:50:59--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.222.48, 52.217.69.246, 54.231.166.168, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.222.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1242874899 (1.2G) [application/x-tar]\n",
            "Saving to: ‘bert-large-cased.tar.gz’\n",
            "\n",
            "bert-large-cased.ta 100%[===================>]   1.16G  13.3MB/s    in 91s     \n",
            "\n",
            "2024-04-23 16:52:31 (13.1 MB/s) - ‘bert-large-cased.tar.gz’ saved [1242874899/1242874899]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf /content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD41p08J3kon",
        "outputId": "87292f50-4665-453f-c099-2b390734633f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_config.json\n",
            "pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow tensorflow-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x21Nhagg6g_P",
        "outputId": "a6cb03ea-ac20-4228-c5cc-455d407d66dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.9.0 --default-timeout=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5PPGHAqQ6qfN",
        "outputId": "27ae0282-34ec-490b-a50a-267bd35847a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==2.9.0\n",
            "  Downloading tensorflow_gpu-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow-gpu==2.9.0)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-gpu==2.9.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (1.62.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (3.9.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow-gpu==2.9.0)\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow-gpu==2.9.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow-gpu==2.9.0)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (0.36.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow-gpu==2.9.0)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu==2.9.0) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow-gpu==2.9.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (3.6)\n",
            "Collecting protobuf>=3.9.2 (from tensorflow-gpu==2.9.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9.0) (3.2.2)\n",
            "Installing collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow-gpu\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-estimator-2.9.0 tensorflow-gpu-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flatbuffers",
                  "gast",
                  "google",
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "fdf90dc5bbf9471c919a97a816575319"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorboardX\n",
        "!pip install tensorboardX\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVviIsuY8rPl",
        "outputId": "019c7a51-37f3-499b-ae8c-52f82c904162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorboardX 2.6.2.2\n",
            "Uninstalling tensorboardX-2.6.2.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorboardX-2.6.2.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorboardX/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorboardX-2.6.2.2\n",
            "Collecting tensorboardX\n",
            "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Collecting protobuf>=3.20 (from tensorboardX)\n",
            "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, tensorboardX\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.4.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.48.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-bigquery 3.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.26.1 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.26.1 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.26.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 5.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-5.26.1 tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT_BASE_DIR=bert-base-chinese\n",
        "#DATA_DIR= '/content/BERT-BiLSTM-CRF-NER-pytorch/data'\n",
        "#OUTPUT_DIR= '/content/BERT-BiLSTM-CRF-NER-pytorch/output/'\n",
        "!export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "!python ner.py \\\n",
        "    --model_name_or_path /content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased/ \\\n",
        "    --do_train True \\\n",
        "    --do_eval True \\\n",
        "    --do_test True \\\n",
        "    --max_seq_length 256 \\\n",
        "    --train_file /content/BERT-BiLSTM-CRF-NER-pytorch/data/train.txt \\\n",
        "    --eval_file /content/BERT-BiLSTM-CRF-NER-pytorch/data/dev.txt \\\n",
        "    --test_file /content/BERT-BiLSTM-CRF-NER-pytorch/data/test.txt \\\n",
        "    --train_batch_size 32 \\\n",
        "    --eval_batch_size 32 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --do_lower_case \\\n",
        "    --logging_steps 200 \\\n",
        "    --need_birnn True \\\n",
        "    --rnn_dim 256 \\\n",
        "    --clean True \\\n",
        "    --output_dir /content/BERT-BiLSTM-CRF-NER-pytorch/output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsArMBTsxFpP",
        "outputId": "3ca07489-847a-4d88-f598-e24224f11982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04/23/2024 17:27:36 - INFO - __main__ -   device: cuda n_gpu: 1\n",
            "/content/BERT-BiLSTM-CRF-NER-pytorch/output/eval\n",
            "/content/BERT-BiLSTM-CRF-NER-pytorch/output/eval/events.out.tfevents.1713892435.3f45274f8062\n",
            "/content/BERT-BiLSTM-CRF-NER-pytorch/output/label2id.pkl\n",
            "04/23/2024 17:27:36 - INFO - utils -   loading labels info from train file and dump in /content/BERT-BiLSTM-CRF-NER-pytorch/output/\n",
            "04/23/2024 17:27:36 - INFO - utils -   loading error and return the default labels B,I,O\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.tokenization_utils -   Model name '/content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased/' is a path or url to a directory containing tokenizer files.\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased/added_tokens.json. We won't load it.\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased/special_tokens_map.json. We won't load it.\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased/tokenizer_config.json. We won't load it.\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.tokenization_utils -   loading file /content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased/vocab.txt\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased/config.json\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "04/23/2024 17:27:36 - INFO - pytorch_transformers.modeling_utils -   loading weights file /content/BERT-BiLSTM-CRF-NER-pytorch/bert-large-cased/pytorch_model.bin\n",
            "04/23/2024 17:27:43 - INFO - pytorch_transformers.modeling_utils -   Weights of BERT_BiLSTM_CRF not initialized from pretrained model: ['birnn.weight_ih_l0', 'birnn.weight_hh_l0', 'birnn.bias_ih_l0', 'birnn.bias_hh_l0', 'birnn.weight_ih_l0_reverse', 'birnn.weight_hh_l0_reverse', 'birnn.bias_ih_l0_reverse', 'birnn.bias_hh_l0_reverse', 'hidden2tag.weight', 'hidden2tag.bias', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions']\n",
            "04/23/2024 17:27:43 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BERT_BiLSTM_CRF: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "convert examples: 0it [00:00, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/BERT-BiLSTM-CRF-NER-pytorch/ner.py\", line 366, in <module>\n",
            "    main()\n",
            "  File \"/content/BERT-BiLSTM-CRF-NER-pytorch/ner.py\", line 222, in main\n",
            "    train_sampler = RandomSampler(train_data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\", line 143, in __init__\n",
            "    raise ValueError(f\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\")\n",
            "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2024.4.23 plan2:"
      ],
      "metadata": {
        "id": "pcFZzzIYY1_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/liuyukid/transformers-ner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt6pRSdgY4sh",
        "outputId": "e91c4754-fa97-4f0a-93b5-1f86b1c2f923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers-ner'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 124 (delta 21), reused 18 (delta 18), pack-reused 97\u001b[K\n",
            "Receiving objects: 100% (124/124), 1.71 MiB | 17.31 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.20.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UweJaNr1bjhw",
        "outputId": "d8f72ff0-af7b-4b47-992c-a413a50f649a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.20.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.20.1)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (2024.6.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "id": "T00h-BsXb_PV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90bb7414-4c39-4d47-bfae-e8d6010ba4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/transformers-ner/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mxU1CqpcPF4",
        "outputId": "8b932081-543e-481c-bb3c-2c83351e8521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers-ner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M--p1E_9iqou",
        "outputId": "36f7c731-1fd3-4ebc-d9c7-3782b194d055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers-ner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "utils_init_path = os.path.join('utils', '__init__.py')\n",
        "if not os.path.exists(utils_init_path):\n",
        "    open(utils_init_path, 'a').close()\n"
      ],
      "metadata": {
        "id": "Bcg4RtZXjaOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "R4bniayjj66f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c164b4-6f1d-41b8-f67b-10fd15570867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=690eb54f646ae428866290ea2329e8b7dc9eeb9602edc3c7b8c68ee1bce59725\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/transformers-ner/output/impacts"
      ],
      "metadata": {
        "id": "m5DMcRVlGv2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/transformers-ner/output/impacts"
      ],
      "metadata": {
        "id": "o8Ass-FnGyfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/transformers-ner/output/ncbi"
      ],
      "metadata": {
        "id": "eDS7IterWELn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/transformers-ner/output/ncbi"
      ],
      "metadata": {
        "id": "SINCEWnxWEi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/transformers-ner/output/mimic"
      ],
      "metadata": {
        "id": "R64ILm4TSSbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/transformers-ner/output/mimic"
      ],
      "metadata": {
        "id": "xVMJBk59SU-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/transformers-ner/datasets/mimic/cached_dev_bert-large-cased_128\n",
        "!rm /content/transformers-ner/datasets/mimic/cached_test_bert-large-cased_128\n",
        "!rm /content/transformers-ner/datasets/mimic/cached_train_bert-large-cased_128"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CDBqpyAQVwNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/transformers-ner/datasets/mimic/cached_dev_xlm-roberta-base_128\n",
        "!rm /content/transformers-ner/datasets/mimic/cached_test_xlm-roberta-base_128\n",
        "!rm /content/transformers-ner/datasets/mimic/cached_train_xlm-roberta-base_128"
      ],
      "metadata": {
        "id": "BMBxSKw5v8Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/transformers-ner/output/bc5cdr\n",
        "!mkdir /content/transformers-ner/output/bc5cdr"
      ],
      "metadata": {
        "id": "ed-z3GQ-qE3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/transformers-ner/datasets/bc5cdr/cached_dev_bert-large-cased_128\n",
        "!rm /content/transformers-ner/datasets/bc5cdr/cached_test_bert-large-cased_128\n",
        "!rm /content/transformers-ner/datasets/bc5cdr/cached_train_bert-large-cased_128"
      ],
      "metadata": {
        "id": "AQegcnhGAqY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/transformers-ner/datasets/bc5cdr/cached_dev_xlm-roberta-base_128\n",
        "!rm /content/transformers-ner/datasets/bc5cdr/cached_test_xlm-roberta-base_128\n",
        "!rm /content/transformers-ner/datasets/bc5cdr/cached_train_xlm-roberta-base_128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I46CSX1U8a2z",
        "outputId": "a0bb6006-0aa2-4320-9dfa-e47c1760779a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/transformers-ner/datasets/bc5cdr/cached_test_xlm-roberta-base_128': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/transformers-ner/output/med\n",
        "!mkdir /content/transformers-ner/output/med"
      ],
      "metadata": {
        "id": "u_S-u2pt7_Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/transformers-ner/datasets/med/cached_dev_bert-large-cased_128\n",
        "!rm /content/transformers-ner/datasets/med/cached_test_bert-large-cased_128\n",
        "!rm /content/transformers-ner/datasets/med/cached_train_bert-large-cased_128"
      ],
      "metadata": {
        "id": "XWESyt4q8Ch1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sh ./scripts/run_softmax_ner.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-7M_uVKb-Oa",
        "outputId": "41a09902-cef1-425d-a4fc-07d69040873b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-07 23:33:55.290404: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-07 23:33:55.290449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-07 23:33:55.291951: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-07 23:33:55.299610: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-07 23:33:56.363876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "06/07/2024 23:33:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "06/07/2024 23:33:58 - INFO - __main__ -   Tokenizer arguments: {'do_lower_case': False}\n",
            "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertSoftmaxForNer: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertSoftmaxForNer from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertSoftmaxForNer from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertSoftmaxForNer were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "06/07/2024 23:34:05 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/content/transformers-ner/datasets/med', model_type='bert', model_name_or_path='bert-large-cased', output_dir='/content/transformers-ner/output/med', labels='/content/transformers-ner/datasets/med/labels.txt', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, do_lower_case=False, keep_accents=None, strip_accents=None, use_fast=None, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=32, gradient_accumulation_steps=1, loss_type='lsr', learning_rate=5e-05, bert_lr=5e-05, classifier_lr=5e-05, adv_training='fgm', weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps='0.5', eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'))\n",
            "06/07/2024 23:34:05 - INFO - __main__ -   Creating features from dataset file at /content/transformers-ner/datasets/med\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   Writing example 0 of 119\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   *** Example ***\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   guid: train-1\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   tokens: [CLS] Complex patterns of multiple bio ##mine ##ral ##ization in single - cell ##ed plant t ##rich ##ome ##s of the Lo ##asa ##ceae Plants of the family Lo ##asa ##ceae are characterized by a usually dense in ##du ##ment of various t ##rich ##ome types , including two basically different types mineral ##ized , un ##ice ##ll ##ular t ##rich ##ome ##s ( stinging hairs or set ##ae and s ##ca ##bri ##d - g ##loc ##hi ##dia ##te t ##rich ##ome ##s ) . [SEP]\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   valid_mask: 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_ids: 101 9974 6692 1104 2967 25128 9685 4412 2734 1107 1423 118 2765 1174 2582 189 10886 6758 1116 1104 1103 10605 18384 8814 25880 1104 1103 1266 10605 18384 8814 1132 6858 1118 170 1932 9613 1107 7641 1880 1104 1672 189 10886 6758 3322 117 1259 1160 11519 1472 3322 10956 2200 117 8362 4396 2339 5552 189 10886 6758 1116 113 25648 14087 1137 1383 5024 1105 188 2599 27647 1181 118 176 27089 3031 7168 1566 189 10886 6758 1116 114 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   label_ids: -100 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   start_ids: -100 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   end_ids: -100 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   *** Example ***\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   guid: train-2\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   tokens: [CLS] By comparing Cell ##S ##ear ##ch and fi ##ltration ( IS ##ET ) - en ##rich ##ment combined to four color im ##mu ##no ##f ##lu ##ores ##cent stain ##ing , we showed that Cell ##S ##ear ##ch and IS ##ET isolated distinct sub ##pop ##ulation ##s of CT ##Cs : CT ##Cs undergoing e ##pit ##hel ##ial - to - me ##sen ##chy ##mal transition , CT ##C clusters and large CT ##Cs with c ##yt ##omo ##rp ##hol ##ogical characteristics but no detect ##able markers were isolated using IS ##ET . [SEP]\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   valid_mask: 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_ids: 101 1650 15089 17369 1708 19386 1732 1105 20497 21875 113 19432 11943 114 118 4035 10886 1880 3490 1106 1300 2942 13280 13601 2728 2087 7535 12238 8298 24754 1158 117 1195 2799 1115 17369 1708 19386 1732 1105 19432 11943 6841 4966 4841 18299 6856 1116 1104 16899 18363 131 16899 18363 15819 174 18965 18809 2916 118 1106 118 1143 3792 8992 7435 6468 117 16899 1658 13687 1105 1415 16899 18363 1114 172 25669 18445 15615 14084 20946 5924 1133 1185 11552 1895 18004 1127 6841 1606 19432 11943 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   label_ids: -100 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   start_ids: -100 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   end_ids: -100 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   *** Example ***\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   guid: train-3\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   tokens: [CLS] On a B ##land - Alt ##man plot , the bias and precision between these two methods was 19 . 8 ± 16 . 7 ( Lim ##its of agreement - 20 . 1 to 59 . 6 ) mm ##H ##g , S ##pear ##man correlation coefficient r = 0 . 61 . [SEP]\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   valid_mask: 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_ids: 101 1212 170 139 1931 118 14983 1399 4928 117 1103 15069 1105 13218 1206 1292 1160 4069 1108 1627 119 129 212 1479 119 128 113 21551 6439 1104 3311 118 1406 119 122 1106 4589 119 127 114 2608 3048 1403 117 156 25745 1399 18741 21130 187 134 121 119 5391 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   label_ids: -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   start_ids: -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   end_ids: -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   *** Example ***\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   guid: train-4\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   tokens: [CLS] Furthermore , with both the presence of a potent anti ##ox ##ida ##nt ( Re ##s ##ver ##at ##rol ) and down ##re ##gu ##lation of the anti ##ox ##ida ##nt - related gene P ##r ##x - 1 ( Per ##ox ##ired ##ox ##in - 1 ) , E ##LF - E ##MF was associated with higher inflammatory responses of mac ##rop ##hage ##s . [SEP]\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   valid_mask: 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_ids: 101 7282 117 1114 1241 1103 2915 1104 170 18106 2848 10649 6859 2227 113 11336 1116 4121 2980 13166 114 1105 1205 1874 13830 6840 1104 1103 2848 10649 6859 2227 118 2272 5565 153 1197 1775 118 122 113 14286 10649 24599 10649 1394 118 122 114 117 142 25470 118 142 18586 1108 2628 1114 2299 22653 11317 1104 23639 12736 19911 1116 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   label_ids: -100 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   start_ids: -100 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   end_ids: -100 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   *** Example ***\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   guid: train-5\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   tokens: [CLS] We also proved that CC ##R ##2 high - expressed MS ##C - ex ##o could reduce the concentration of free CC ##L ##2 and suppress its functions to recruit or activate mac ##rop ##hage . [SEP]\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   valid_mask: 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_ids: 101 1284 1145 4132 1115 21362 2069 1477 1344 118 4448 10978 1658 118 4252 1186 1180 4851 1103 6256 1104 1714 21362 2162 1477 1105 17203 1157 4226 1106 14240 1137 23162 23639 12736 19911 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   label_ids: -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   start_ids: -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "06/07/2024 23:34:05 - INFO - utils.utils_ner -   end_ids: -100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers-ner/./examples/run_softmax_ner.py\", line 732, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers-ner/./examples/run_softmax_ner.py\", line 669, in main\n",
            "    train_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=\"train\")\n",
            "  File \"/content/transformers-ner/./examples/run_softmax_ner.py\", line 396, in load_and_cache_examples\n",
            "    features = convert_examples_to_features(\n",
            "  File \"/content/transformers-ner/utils/utils_ner.py\", line 112, in convert_examples_to_features\n",
            "    label_ids = [label_map[label] for label in example.labels]\n",
            "  File \"/content/transformers-ner/utils/utils_ner.py\", line 112, in <listcomp>\n",
            "    label_ids = [label_map[label] for label in example.labels]\n",
            "KeyError: 'B-SpecificDisease'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sh ./scripts/run_crf_ner.sh"
      ],
      "metadata": {
        "id": "kMk6IBIL3uPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/transformers-ner-new.zip /content/transformers-ner"
      ],
      "metadata": {
        "id": "rUHhE1A4wekP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XsETinJxw0zN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb00cf60-c619-4352-ed1b-8029db6d1767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/transformers-ner-new.zip /content/drive/MyDrive/transformers-ner-new.zip"
      ],
      "metadata": {
        "id": "G-O8IVTOxqs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gwjdb1SFT88",
        "outputId": "f359fd59-a8d9-44ef-9bc7-e3daf778b8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/transformers-ner-new.zip"
      ],
      "metadata": {
        "id": "dbm6YXrQ0uEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b4a2c6-8a11-43de-cab2-efa9ca211a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/transformers-ner-new.zip\n",
            "   creating: content/transformers-ner/\n",
            "   creating: content/transformers-ner/.git/\n",
            " extracting: content/transformers-ner/.git/HEAD  \n",
            "  inflating: content/transformers-ner/.git/packed-refs  \n",
            "  inflating: content/transformers-ner/.git/index  \n",
            "   creating: content/transformers-ner/.git/refs/\n",
            "   creating: content/transformers-ner/.git/refs/remotes/\n",
            "   creating: content/transformers-ner/.git/refs/remotes/origin/\n",
            " extracting: content/transformers-ner/.git/refs/remotes/origin/HEAD  \n",
            "   creating: content/transformers-ner/.git/refs/tags/\n",
            "   creating: content/transformers-ner/.git/refs/heads/\n",
            " extracting: content/transformers-ner/.git/refs/heads/master  \n",
            "   creating: content/transformers-ner/.git/objects/\n",
            "   creating: content/transformers-ner/.git/objects/pack/\n",
            "  inflating: content/transformers-ner/.git/objects/pack/pack-27491e95729238447297cbcad19ddaaf44924491.idx  \n",
            "  inflating: content/transformers-ner/.git/objects/pack/pack-27491e95729238447297cbcad19ddaaf44924491.pack  \n",
            "   creating: content/transformers-ner/.git/objects/info/\n",
            "  inflating: content/transformers-ner/.git/config  \n",
            "   creating: content/transformers-ner/.git/info/\n",
            "  inflating: content/transformers-ner/.git/info/exclude  \n",
            "   creating: content/transformers-ner/.git/branches/\n",
            "   creating: content/transformers-ner/.git/logs/\n",
            "  inflating: content/transformers-ner/.git/logs/HEAD  \n",
            "   creating: content/transformers-ner/.git/logs/refs/\n",
            "   creating: content/transformers-ner/.git/logs/refs/remotes/\n",
            "   creating: content/transformers-ner/.git/logs/refs/remotes/origin/\n",
            "  inflating: content/transformers-ner/.git/logs/refs/remotes/origin/HEAD  \n",
            "   creating: content/transformers-ner/.git/logs/refs/heads/\n",
            "  inflating: content/transformers-ner/.git/logs/refs/heads/master  \n",
            "  inflating: content/transformers-ner/.git/description  \n",
            "   creating: content/transformers-ner/.git/hooks/\n",
            "  inflating: content/transformers-ner/.git/hooks/commit-msg.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/push-to-checkout.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/pre-commit.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/pre-rebase.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/update.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/pre-receive.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/pre-push.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/post-update.sample  \n",
            "  inflating: content/transformers-ner/.git/hooks/pre-merge-commit.sample  \n",
            "   creating: content/transformers-ner/scripts/\n",
            "  inflating: content/transformers-ner/scripts/run_span_ner.sh  \n",
            "  inflating: content/transformers-ner/scripts/run_crf_ner.sh  \n",
            "  inflating: content/transformers-ner/scripts/run_softmax_ner.sh  \n",
            "   creating: content/transformers-ner/scripts/.ipynb_checkpoints/\n",
            "   creating: content/transformers-ner/examples/\n",
            "  inflating: content/transformers-ner/examples/run_softmax_ner.py  \n",
            "  inflating: content/transformers-ner/examples/run_span_ner.py  \n",
            "  inflating: content/transformers-ner/examples/run_crf_ner.py  \n",
            "   creating: content/transformers-ner/output/\n",
            "   creating: content/transformers-ner/output/mimic/\n",
            "   creating: content/transformers-ner/output/bc5cdr/\n",
            "  inflating: content/transformers-ner/output/bc5cdr/vocab.txt  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/config.json  \n",
            "   creating: content/transformers-ner/output/bc5cdr/best_checkpoint/\n",
            "  inflating: content/transformers-ner/output/bc5cdr/best_checkpoint/vocab.txt  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/best_checkpoint/config.json  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/best_checkpoint/scheduler.pt  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/best_checkpoint/training_args.bin  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/best_checkpoint/optimizer.pt  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/best_checkpoint/tokenizer_config.json  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/best_checkpoint/special_tokens_map.json  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/best_checkpoint/pytorch_model.bin  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/best_checkpoint/tokenizer.json  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/training_args.bin  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/eval_results.txt  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/events.out.tfevents.1713917372.df82345c6394.77910.0  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/tokenizer_config.json  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/test_results.txt  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/special_tokens_map.json  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/test_predictions.txt  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/pytorch_model.bin  \n",
            "  inflating: content/transformers-ner/output/bc5cdr/tokenizer.json  \n",
            "   creating: content/transformers-ner/output/impacts/\n",
            "  inflating: content/transformers-ner/output/impacts/vocab.txt  \n",
            "  inflating: content/transformers-ner/output/impacts/config.json  \n",
            "  inflating: content/transformers-ner/output/impacts/training_args.bin  \n",
            "  inflating: content/transformers-ner/output/impacts/eval_results.txt  \n",
            "  inflating: content/transformers-ner/output/impacts/tokenizer_config.json  \n",
            "  inflating: content/transformers-ner/output/impacts/special_tokens_map.json  \n",
            "  inflating: content/transformers-ner/output/impacts/pytorch_model.bin  \n",
            "  inflating: content/transformers-ner/output/impacts/events.out.tfevents.1713924655.df82345c6394.108662.0  \n",
            "  inflating: content/transformers-ner/output/impacts/tokenizer.json  \n",
            "   creating: content/transformers-ner/output/.ipynb_checkpoints/\n",
            "   creating: content/transformers-ner/.idea/\n",
            "  inflating: content/transformers-ner/.idea/misc.xml  \n",
            "   creating: content/transformers-ner/.idea/inspectionProfiles/\n",
            "  inflating: content/transformers-ner/.idea/inspectionProfiles/Project_Default.xml  \n",
            "  inflating: content/transformers-ner/.idea/inspectionProfiles/profiles_settings.xml  \n",
            "  inflating: content/transformers-ner/.idea/transformer-ner.iml  \n",
            "  inflating: content/transformers-ner/.idea/modules.xml  \n",
            "  inflating: content/transformers-ner/.idea/vcs.xml  \n",
            "  inflating: content/transformers-ner/.idea/.gitignore  \n",
            "  inflating: content/transformers-ner/README.md  \n",
            "   creating: content/transformers-ner/utils/\n",
            "  inflating: content/transformers-ner/utils/utils_metrics.py  \n",
            "   creating: content/transformers-ner/utils/__pycache__/\n",
            "  inflating: content/transformers-ner/utils/__pycache__/utils_ner.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/utils/__pycache__/utils_adversarial.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/utils/__pycache__/utils_metrics.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/utils/__pycache__/__init__.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/utils/utils_adversarial.py  \n",
            " extracting: content/transformers-ner/utils/__init__.py  \n",
            "  inflating: content/transformers-ner/utils/utils_ner.py  \n",
            "   creating: content/transformers-ner/models/\n",
            "  inflating: content/transformers-ner/models/electra_ner.py  \n",
            "  inflating: content/transformers-ner/models/xlm_ner.py  \n",
            "  inflating: content/transformers-ner/models/roberta_ner.py  \n",
            "  inflating: content/transformers-ner/models/model_ner.py  \n",
            "   creating: content/transformers-ner/models/__pycache__/\n",
            "  inflating: content/transformers-ner/models/__pycache__/model_utils.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/__pycache__/albert_ner.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/__pycache__/roberta_ner.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/__pycache__/electra_ner.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/__pycache__/bert_ner.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/__pycache__/distilbert_ner.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/__pycache__/xlm_ner.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/__pycache__/model_ner.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/distilbert_ner.py  \n",
            "   creating: content/transformers-ner/models/layers/\n",
            "   creating: content/transformers-ner/models/layers/__pycache__/\n",
            "  inflating: content/transformers-ner/models/layers/__pycache__/linears.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/layers/linears.py  \n",
            "   creating: content/transformers-ner/models/losses/\n",
            "   creating: content/transformers-ner/models/losses/__pycache__/\n",
            "  inflating: content/transformers-ner/models/losses/__pycache__/label_smoothing.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/losses/__pycache__/focal_loss.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/losses/__pycache__/crf.cpython-310.pyc  \n",
            "  inflating: content/transformers-ner/models/losses/label_smoothing.py  \n",
            "  inflating: content/transformers-ner/models/losses/dice_loss.py  \n",
            "  inflating: content/transformers-ner/models/losses/focal_loss.py  \n",
            "  inflating: content/transformers-ner/models/losses/crf.py  \n",
            "  inflating: content/transformers-ner/models/bert_ner.py  \n",
            "  inflating: content/transformers-ner/models/model_utils.py  \n",
            "  inflating: content/transformers-ner/models/albert_ner.py  \n",
            "   creating: content/transformers-ner/.ipynb_checkpoints/\n",
            "   creating: content/transformers-ner/datasets/\n",
            "   creating: content/transformers-ner/datasets/mimic/\n",
            "  inflating: content/transformers-ner/datasets/mimic/labels.txt  \n",
            "  inflating: content/transformers-ner/datasets/mimic/test.txt  \n",
            "  inflating: content/transformers-ner/datasets/mimic/dev.txt  \n",
            "  inflating: content/transformers-ner/datasets/mimic/train.txt  \n",
            "   creating: content/transformers-ner/datasets/cluener/\n",
            "  inflating: content/transformers-ner/datasets/cluener/labels.txt  \n",
            "  inflating: content/transformers-ner/datasets/cluener/dev.txt  \n",
            "  inflating: content/transformers-ner/datasets/cluener/train.txt  \n",
            "   creating: content/transformers-ner/datasets/bc5cdr/\n",
            "  inflating: content/transformers-ner/datasets/bc5cdr/cached_train_bert-large-cased_128  \n",
            "  inflating: content/transformers-ner/datasets/bc5cdr/labels.txt  \n",
            "  inflating: content/transformers-ner/datasets/bc5cdr/test.txt  \n",
            "  inflating: content/transformers-ner/datasets/bc5cdr/cached_dev_bert-large-cased_128  \n",
            "   creating: content/transformers-ner/datasets/bc5cdr/.ipynb_checkpoints/\n",
            "  inflating: content/transformers-ner/datasets/bc5cdr/dev.txt  \n",
            "  inflating: content/transformers-ner/datasets/bc5cdr/cached_test_bert-large-cased_128  \n",
            "  inflating: content/transformers-ner/datasets/bc5cdr/train.txt  \n",
            "   creating: content/transformers-ner/datasets/impacts/\n",
            "  inflating: content/transformers-ner/datasets/impacts/cached_train_bert-large-cased_128  \n",
            "  inflating: content/transformers-ner/datasets/impacts/labels.txt  \n",
            "  inflating: content/transformers-ner/datasets/impacts/test.txt  \n",
            "  inflating: content/transformers-ner/datasets/impacts/cached_dev_bert-large-cased_128  \n",
            "   creating: content/transformers-ner/datasets/impacts/.ipynb_checkpoints/\n",
            "  inflating: content/transformers-ner/datasets/impacts/dev.txt  \n",
            "  inflating: content/transformers-ner/datasets/impacts/train.txt  \n",
            "   creating: content/transformers-ner/datasets/conll2003/\n",
            "  inflating: content/transformers-ner/datasets/conll2003/labels.txt  \n",
            "  inflating: content/transformers-ner/datasets/conll2003/test.txt  \n",
            "   creating: content/transformers-ner/datasets/conll2003/.ipynb_checkpoints/\n",
            "  inflating: content/transformers-ner/datasets/conll2003/dev.txt  \n",
            "  inflating: content/transformers-ner/datasets/conll2003/train.txt  \n",
            "   creating: content/transformers-ner/datasets/.ipynb_checkpoints/\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}